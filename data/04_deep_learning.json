{
  "chapter_id": "04",
  "chapter_title": "ディープラーニングの概要",
  "terms": [
    {
      "term": "ニューラルネットワーク",
      "reading": "にゅーらるねっとわーく",
      "definition": "人間の脳の神経細胞（ニューロン）をまねた計算モデル。情報を層ごとに少しずつ変換しながら次へ渡す仕組み。",
      "example": "手書き数字を見て「これは5」と判断する。"
    },
    {
      "term": "ディープラーニング",
      "reading": "でぃーぷらーにんぐ",
      "definition": "ニューラルネットワークを多層にして、特徴を自動で学習できるようにした方法。",
      "example": "顔認識、音声認識、自動運転などで使われる。"
    },
    {
      "term": "CPU",
      "reading": "しーぴーゆー",
      "definition": "パソコンの頭脳。何でもできるが、大量の同時処理は苦手。",
      "example": "一般的な計算処理に使われる。"
    },
    {
      "term": "GPU",
      "reading": "じーぴーゆー",
      "definition": "画像処理用に作られた高速計算チップ。同じ計算を大量に同時に行うのが得意で、ディープラーニングと相性が良い。",
      "example": "AIの学習を高速化するために使われる。"
    },
    {
      "term": "TPU",
      "reading": "てぃーぴーゆー",
      "definition": "Googleが開発したAI専用チップ。ディープラーニングの計算に特化して高速に動作する。",
      "example": "GoogleのAIサービスで利用される。"
    },
    {
      "term": "パーセプトロン",
      "reading": "ぱーせぷとろん",
      "definition": "最も基本的なニューラルネットワークで、入力→計算→出力だけのシンプルな構造。",
      "example": "簡単な分類問題に使われる。"
    },
    {
      "term": "多層パーセプトロン（MLP）",
      "reading": "たそうぱーせぷとろん",
      "definition": "パーセプトロンを複数の層に重ねたモデル。より複雑なパターンを学習できる。",
      "example": "画像や音声の分類に使われる。"
    },
    {
      "term": "活性化関数",
      "reading": "かっせいかかんすう",
      "definition": "ニューロンがどれくらい反応するかを決める関数。学習の非線形性を生む重要な要素。",
      "example": "ReLU、シグモイド、tanh、ソフトマックスなど。"
    },
    {
      "term": "シグモイド関数",
      "reading": "しぐもいどかんすう",
      "definition": "0〜1の間で滑らかに変化する活性化関数。",
      "example": "確率のような値を出したいときに使われる。"
    },
    {
      "term": "ReLU",
      "reading": "れる",
      "definition": "0以下は0、0より大きいとそのまま出力する活性化関数。計算が速く、現在最もよく使われる。",
      "example": "深いニューラルネットワークで一般的に使われる。"
    },
    {
      "term": "ソフトマックス関数",
      "reading": "そふとまっくすかんすう",
      "definition": "出力を確率に変換する活性化関数。分類問題の最後に使われる。",
      "example": "「犬：80%、猫：20%」のように確率で出力する。"
    },
    {
      "term": "誤差関数（損失関数）",
      "reading": "ごさかんすう",
      "definition": "AIの間違いの大きさを数値で表す関数。",
      "example": "分類では交差エントロピー、回帰では平均二乗誤差が使われる。"
    },
    {
      "term": "誤差逆伝播法（バックプロパゲーション）",
      "reading": "ごさぎゃくでんぱほう",
      "definition": "誤差をもとに重みを調整し、AIを賢くする学習方法。",
      "example": "出力側から入力側へ誤差を伝えて修正する。"
    },
    {
      "term": "勾配消失",
      "reading": "こうばいしょうしつ",
      "definition": "誤差が小さくなりすぎて学習が進まなくなる問題。",
      "example": "深いネットワークでシグモイドを使うと起きやすい。"
    },
    {
      "term": "勾配爆発",
      "reading": "こうばいばくはつ",
      "definition": "誤差が大きくなりすぎて学習が不安定になる問題。",
      "example": "RNNで起きやすい。"
    },
    {
      "term": "最適化アルゴリズム（Optimizer）",
      "reading": "さいてきかあるごりずむ",
      "definition": "AIの重みをどう調整するかを決める学習方法。",
      "example": "SGD、Momentum、Adamなど。"
    },
    {
      "term": "ハイパーパラメータ",
      "reading": "はいぱーぱらめーた",
      "definition": "AIの学習方法を決める設定値。人間が事前に決める必要がある。",
      "example": "学習率、エポック数、バッチサイズなど。"
    },
    {
      "term": "エポック（Epoch）",
      "reading": "えぽっく",
      "definition": "学習データを1周すること。",
      "example": "10エポックなら、データを10回学習したことになる。"
    },
    {
      "term": "バッチ学習",
      "reading": "ばっちがくしゅう",
      "definition": "全データをまとめて学習する方法。",
      "example": "データ量が少ない場合に使われる。"
    },
    {
      "term": "ミニバッチ学習",
      "reading": "みにばっちがくしゅう",
      "definition": "データを小分けにして学習する方法。最も一般的。",
      "example": "32件ずつ学習するなど。"
    },
    {
      "term": "オンライン学習",
      "reading": "おんらいんがくしゅう",
      "definition": "データを1件ずつリアルタイムで学習する方法。",
      "example": "ストリーミングデータの分析に使われる。"
    },
    {
      "term": "過学習（オーバーフィッティング）",
      "reading": "かがくしゅう",
      "definition": "訓練データに合わせすぎて、新しいデータで失敗する状態。",
      "example": "テストの答えだけ覚えて応用ができない状態。"
    },
    {
      "term": "正則化",
      "reading": "せいそくか",
      "definition": "過学習を防ぐための工夫。",
      "example": "L1/L2正則化、ドロップアウトなど。"
    },
    {
      "term": "早期終了（Early Stopping）",
      "reading": "そうきしゅうりょう",
      "definition": "過学習しそうになったら学習を止める方法。",
      "example": "検証データの誤差が悪化したらストップする。"
    },
    {
      "term": "ノーフリーランチ定理",
      "reading": "のーふりーらんちていり",
      "definition": "どんな問題にも最強のアルゴリズムは存在しないという考え方。",
      "example": "問題によって最適な手法は異なる。"
    }
  ]
}
